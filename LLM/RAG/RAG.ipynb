{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muniappabalaji/TimeSeries/blob/main/LLM/RAG/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q  langchain langchain_community langchain_experimental langchain_openai pdfplumber faiss-cpu"
      ],
      "metadata": {
        "id": "AiAEdObM_KBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/deepanrajm/GL.git\n",
        "!git clone https://github.com/muniappabalaji/TimeSeries.git"
      ],
      "metadata": {
        "id": "rLiFrdMs_Nl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnpYzL6_-9PH"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PDFPlumberLoader\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGgGRiVo-9PJ"
      },
      "outputs": [],
      "source": [
        "loader = PDFPlumberLoader(r\"/content/GL/LLM/RAG/Basic_Home_Remedies.pdf\")\n",
        "docs = loader.load()\n",
        "\n",
        "# Check the number of pages\n",
        "print(\"Number of pages in the PDF:\",len(docs))\n",
        "\n",
        "# Load the random page content\n",
        "docs[1].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGxnDNRw-9PK"
      },
      "outputs": [],
      "source": [
        "text_splitter = SemanticChunker(HuggingFaceEmbeddings())\n",
        "documents = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWkvnzMd-9PK"
      },
      "outputs": [],
      "source": [
        "print(\"Number of chunks created: \", len(documents))\n",
        "\n",
        "print(documents[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfxTRBaG-9PK"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,       # size of each chunk\n",
        "    chunk_overlap=50,     # overlap between chunks\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]  # fallback hierarchy\n",
        ")\n",
        "\n",
        "documents = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hn3oEKLH-9PL"
      },
      "outputs": [],
      "source": [
        "print(\"Number of chunks created: \", len(documents))\n",
        "\n",
        "print(documents[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tXCwFDC-9PL"
      },
      "outputs": [],
      "source": [
        "# Instantiate the embedding model\n",
        "embedder = HuggingFaceEmbeddings()\n",
        "\n",
        "# Create the vector store\n",
        "vector = FAISS.from_documents(documents, embedder)\n",
        "retriever = vector.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDCVAE7L-9PL"
      },
      "outputs": [],
      "source": [
        "# from langchain_community.llms import Ollama\n",
        "\n",
        "# llm = Ollama(model=\"llama3.2:1b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngZJv396-9PL"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "llm = ChatOpenAI(openai_api_base = \"https://openrouter.ai/api/v1\", openai_api_key = \"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\", model = \"deepseek/deepseek-chat-v3.1:free\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ftKiNrO-9PL"
      },
      "outputs": [],
      "source": [
        "# 2. If you don't know the answer, just say that \"I don't know\" but don't make up an answer on your own.\\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfomHGdc-9PM"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = \"\"\"\n",
        "1. You are doctor\n",
        "2. Use the following pieces of context to answer the question at the end.\n",
        "3. Answer only by using the context and articulate it better, use bullet point and emoji if required\n",
        "4. Keep the answer crisp and limited to 3,4 sentences.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer to the question:\"\"\"\n",
        "\n",
        "\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "                  llm=llm,\n",
        "                  prompt=QA_CHAIN_PROMPT,\n",
        "                  callbacks=None,\n",
        "                  verbose=True)\n",
        "\n",
        "document_prompt = PromptTemplate(\n",
        "    input_variables=[\"page_content\", \"source\"],\n",
        "    template=\"Context:\\ncontent:{page_content}\\nsource:{source}\",\n",
        ")\n",
        "\n",
        "combine_documents_chain = StuffDocumentsChain(\n",
        "                  llm_chain=llm_chain,\n",
        "                  document_variable_name=\"context\",\n",
        "                  document_prompt=document_prompt,\n",
        "                  callbacks=None,\n",
        "              )\n",
        "\n",
        "qa = RetrievalQA(\n",
        "                  combine_documents_chain=combine_documents_chain,\n",
        "                  verbose=True,\n",
        "                  retriever=retriever,\n",
        "                  return_source_documents=True,\n",
        "              )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Af_iCpb-9PM"
      },
      "outputs": [],
      "source": [
        "print(qa(\"remedy for cough?\")[\"result\"])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}