{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muniappabalaji/TimeSeries/blob/main/LLM/RAG/RAG_Pine_Cone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Dn_v6YBDiiFe"
      },
      "outputs": [],
      "source": [
        "!pip install -q pinecone pinecone-client sentence-transformers langchain langchain_community langchain_experimental langchain_openai langchain-pinecone pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/deepanrajm/GL.git\n",
        "!git clone https://github.com/muniappabalaji/TimeSeries.git"
      ],
      "metadata": {
        "id": "A8bWJ0YmiutW",
        "outputId": "7eb5a693-ea69-4495-cf89-26698a324722",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'TimeSeries' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PDFPlumberLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Make sure the PDF path is correct\n",
        "pdf_path = r\"/content/GL/LLM/RAG/Basic_Home_Remedies.pdf\"\n",
        "loader = PDFPlumberLoader(pdf_path)\n",
        "docs = loader.load()\n",
        "\n",
        "print(f\"Number of pages in the PDF: {len(docs)}\")\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
        ")\n",
        "documents = text_splitter.split_documents(docs)\n",
        "print(f\"Number of document chunks created: {len(documents)}\")"
      ],
      "metadata": {
        "id": "8I_aUnvdi0zb",
        "outputId": "9ba4ee04-d2cf-4416-e8e0-c4ef5de5b457",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of pages in the PDF: 3\n",
            "Number of document chunks created: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from google.colab import userdata\n",
        "import time\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_pinecone import Pinecone as PineconeVectorStore\n",
        "\n",
        "# Securely get the API key from Colab secrets\n",
        "try:\n",
        "    PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\"ERROR: PINECONE_API_KEY not found in Colab secrets.\")\n",
        "    print(\"Please follow Step 1 in the instructions to add your key.\")\n",
        "    # Exit the script if the key is not found\n",
        "    exit()\n",
        "\n",
        "index_name = \"Medical-rag\"\n",
        "\n",
        "# Initialize the Pinecone client\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "print(f\"--- Pinecone initialized for index: '{index_name}' ---\")"
      ],
      "metadata": {
        "id": "jCVwJIEijDu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the embedding model (same as your code)\n",
        "embedder = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\n",
        "\n",
        "# Get the dimension of the embeddings\n",
        "embedding_dim = len(embedder.embed_query(\"test query\"))\n",
        "\n",
        "# Create the index if it doesn't already exist\n",
        "index_name = \"medical-rag\" # Changed index name to lowercase and hyphen\n",
        "\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    print(f\"Creating new Pinecone index: {index_name}\")\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=embedding_dim,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud='aws', region='us-east-1')\n",
        "    )\n",
        "    print(\"Index created successfully.\")\n",
        "else:\n",
        "    print(f\"Index '{index_name}' already exists.\")"
      ],
      "metadata": {
        "id": "BYM_VDrMjO-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d777abe2"
      },
      "source": [
        "import os\n",
        "\n",
        "print(\"\\nCreating Pinecone vector store and populating it with documents...\")\n",
        "\n",
        "# Set the Pinecone API key as an environment variable\n",
        "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
        "\n",
        "# The from_documents method will create embeddings and upsert them into the Pinecone index\n",
        "vector_store = PineconeVectorStore.from_documents(\n",
        "    documents,\n",
        "    embedder,\n",
        "    index_name=index_name\n",
        ")\n",
        "\n",
        "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})\n",
        "print(\"Vector store and retriever are ready.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ca47427"
      },
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "llm = ChatOpenAI(openai_api_base = \"https://openrouter.ai/api/v1\", openai_api_key = \"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\", model = \"deepseek/deepseek-chat-v3.1:free\",temperature=0.7) #max_tokens=100\n",
        "\n",
        "# Your custom prompt template\n",
        "prompt = \"\"\"1. You are a doctor.\n",
        "2. Use the following pieces of context to answer the question at the end.\n",
        "3. Answer only by using the context and articulate it better, use bullet points and emoji if required.\n",
        "4. Keep the answer crisp and limited to 3-4 sentences.\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "Answer to the question:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)\n",
        "\n",
        "# Your chain setup\n",
        "llm_chain = LLMChain(llm=llm, prompt=QA_CHAIN_PROMPT, verbose=False)\n",
        "document_prompt = PromptTemplate(\n",
        "    input_variables=[\"page_content\", \"source\"],\n",
        "    template=\"Context:\\ncontent:{page_content}\\nsource:{source}\",\n",
        ")\n",
        "combine_documents_chain = StuffDocumentsChain(\n",
        "    llm_chain=llm_chain,\n",
        "    document_variable_name=\"context\",\n",
        "    document_prompt=document_prompt,\n",
        ")\n",
        "qa = RetrievalQA(\n",
        "    combine_documents_chain=combine_documents_chain,\n",
        "    verbose=True,\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Sending query to the RAG chain ---\")\n",
        "result = qa(\"remedy for cough?\")\n",
        "print(\"\\n--- Final Answer ---\")\n",
        "print(result[\"result\"])\n",
        "print(\"\\n--- Retrieved Documents ---\")\n",
        "for doc in result[\"source_documents\"]:\n",
        "    print(f\"Content: {doc.page_content}\\nSource: {doc.metadata.get('source', 'N/A')}\\n---\")"
      ],
      "metadata": {
        "id": "95PcKlMUkHtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Cleaning up. Deleting the index... ---\")\n",
        "pc.delete_index(index_name)\n",
        "print(f\"Index '{index_name}' has been deleted.\")"
      ],
      "metadata": {
        "id": "SIs4QskOl8GE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}