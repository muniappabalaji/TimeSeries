{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muniappabalaji/TimeSeries/blob/main/LLM/RAG/RAG_Pine_Cone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Dn_v6YBDiiFe"
      },
      "outputs": [],
      "source": [
        "!pip install -q pinecone pinecone-client sentence-transformers langchain langchain_community langchain_experimental langchain_openai langchain-pinecone pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/deepanrajm/GL.git\n",
        "!git clone https://github.com/muniappabalaji/TimeSeries.git"
      ],
      "metadata": {
        "id": "A8bWJ0YmiutW",
        "outputId": "7f67de8f-6e8d-4dce-ed31-11d44bcc2e7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'TimeSeries' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PDFPlumberLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Make sure the PDF path is correct\n",
        "pdf_path = r\"/content/GL/LLM/RAG/Basic_Home_Remedies.pdf\"\n",
        "loader = PDFPlumberLoader(pdf_path)\n",
        "docs = loader.load()\n",
        "\n",
        "print(f\"Number of pages in the PDF: {len(docs)}\")\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
        ")\n",
        "documents = text_splitter.split_documents(docs)\n",
        "print(f\"Number of document chunks created: {len(documents)}\")"
      ],
      "metadata": {
        "id": "8I_aUnvdi0zb",
        "outputId": "f0e39573-27f6-4c8e-c847-dc7f6a7fa92a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of pages in the PDF: 3\n",
            "Number of document chunks created: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from google.colab import userdata\n",
        "import time\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_pinecone import Pinecone as PineconeVectorStore\n",
        "\n",
        "# Securely get the API key from Colab secrets\n",
        "try:\n",
        "    PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\"ERROR: PINECONE_API_KEY not found in Colab secrets.\")\n",
        "    print(\"Please follow Step 1 in the instructions to add your key.\")\n",
        "    # Exit the script if the key is not found\n",
        "    exit()\n",
        "\n",
        "index_name = \"Medical-rag\"\n",
        "\n",
        "# Initialize the Pinecone client\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "print(f\"--- Pinecone initialized for index: '{index_name}' ---\")"
      ],
      "metadata": {
        "id": "jCVwJIEijDu3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "139c5ddf-2a81-4ae3-8681-010c6fef90da"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Pinecone initialized for index: 'Medical-rag' ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the embedding model (same as your code)\n",
        "embedder = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\n",
        "\n",
        "# Get the dimension of the embeddings\n",
        "embedding_dim = len(embedder.embed_query(\"test query\"))\n",
        "\n",
        "# Create the index if it doesn't already exist\n",
        "index_name = \"medical-rag\" # Changed index name to lowercase and hyphen\n",
        "\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    print(f\"Creating new Pinecone index: {index_name}\")\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=embedding_dim,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud='aws', region='us-east-1')\n",
        "    )\n",
        "    print(\"Index created successfully.\")\n",
        "else:\n",
        "    print(f\"Index '{index_name}' already exists.\")"
      ],
      "metadata": {
        "id": "BYM_VDrMjO-x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b94e64a-d21c-4e73-f47e-24bb4445f78e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index 'medical-rag' already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d777abe2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a412b501-e135-48e2-f887-fb75bc256cd5"
      },
      "source": [
        "import os\n",
        "\n",
        "print(\"\\nCreating Pinecone vector store and populating it with documents...\")\n",
        "\n",
        "# Set the Pinecone API key as an environment variable\n",
        "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
        "\n",
        "# The from_documents method will create embeddings and upsert them into the Pinecone index\n",
        "vector_store = PineconeVectorStore.from_documents(\n",
        "    documents,\n",
        "    embedder,\n",
        "    index_name=index_name\n",
        ")\n",
        "\n",
        "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})\n",
        "print(\"Vector store and retriever are ready.\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Creating Pinecone vector store and populating it with documents...\n",
            "Vector store and retriever are ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ca47427"
      },
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "llm = ChatOpenAI(openai_api_base = \"https://openrouter.ai/api/v1\", openai_api_key = \"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\", model = \"deepseek/deepseek-chat-v3.1:free\",temperature=0.7) #max_tokens=100\n",
        "\n",
        "# Your custom prompt template\n",
        "prompt = \"\"\"1. You are a doctor.\n",
        "2. Use the following pieces of context to answer the question at the end.\n",
        "3. Answer only by using the context and articulate it better, use bullet points and emoji if required.\n",
        "4. Keep the answer crisp and limited to 3-4 sentences.\n",
        "5.Don't say based on the context provided\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "Answer to the question:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)\n",
        "\n",
        "# Your chain setup\n",
        "llm_chain = LLMChain(llm=llm, prompt=QA_CHAIN_PROMPT, verbose=False)\n",
        "document_prompt = PromptTemplate(\n",
        "    input_variables=[\"page_content\", \"source\"],\n",
        "    template=\"Context:\\ncontent:{page_content}\\nsource:{source}\",\n",
        ")\n",
        "combine_documents_chain = StuffDocumentsChain(\n",
        "    llm_chain=llm_chain,\n",
        "    document_variable_name=\"context\",\n",
        "    document_prompt=document_prompt,\n",
        ")\n",
        "qa = RetrievalQA(\n",
        "    combine_documents_chain=combine_documents_chain,\n",
        "    verbose=True,\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        ")"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Sending query to the RAG chain ---\")\n",
        "result = qa(\"remedy for cough?\")\n",
        "print(\"\\n--- Final Answer ---\")\n",
        "print(result[\"result\"])\n",
        "print(\"\\n--- Retrieved Documents ---\")\n",
        "for doc in result[\"source_documents\"]:\n",
        "    print(f\"Content: {doc.page_content}\\nSource: {doc.metadata.get('source', 'N/A')}\\n---\")"
      ],
      "metadata": {
        "id": "95PcKlMUkHtT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f85be9f4-c55d-4d24-fbec-0c0613a4e8e4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sending query to the RAG chain ---\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "--- Final Answer ---\n",
            "Based on the context provided, here are remedies for a cough:\n",
            "\n",
            "- **Honey & Lemon**: Mix one tablespoon of honey with warm water and lemon juice. This soothes the throat and reduces coughing. üçã\n",
            "- **Tulsi (Holy Basil) Tea**: Boil tulsi leaves in water and drink it. This helps with respiratory issues. üåø\n",
            "\n",
            "--- Retrieved Documents ---\n",
            "Content: and sore throat.\n",
            "ÔÇ∑ Turmeric Milk: Mix 1 tsp turmeric in warm milk. Acts as an anti-inflammatory and\n",
            "boosts immunity.\n",
            "ÔÇ∑ Steam Inhalation: Boil water, add eucalyptus oil, and inhale steam to clear nasal\n",
            "congestion.\n",
            "2.2 Cough\n",
            "ÔÇ∑ Honey & Lemon: Mix 1 tbsp honey with warm water and lemon juice. Soothes the\n",
            "throat and reduces cough.\n",
            "ÔÇ∑ Tulsi (Holy Basil) Tea: Boil tulsi leaves in water and drink. Helps with respiratory\n",
            "issues.\n",
            "2.3 Headache\n",
            "ÔÇ∑ Peppermint Oil: Apply a few drops on the temples for relief.\n",
            "Source: /content/GL/LLM/RAG/Basic_Home_Remedies.pdf\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Cleaning up. Deleting the index... ---\")\n",
        "pc.delete_index(index_name)\n",
        "print(f\"Index '{index_name}' has been deleted.\")"
      ],
      "metadata": {
        "id": "SIs4QskOl8GE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a14243a-6bf4-42e5-cfa4-fd174e610c71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Cleaning up. Deleting the index... ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "phGCxDOZ-rlr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}