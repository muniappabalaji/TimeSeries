{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muniappabalaji/TimeSeries/blob/main/LLM/RAG/RAG_Pine_Cone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dn_v6YBDiiFe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e6ee1a2-82b5-47c8-978d-4da6a4fdac33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.6/587.6 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.3/259.3 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q pinecone pinecone-client sentence-transformers langchain langchain_community langchain_experimental langchain_openai langchain-pinecone pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/deepanrajm/GL.git"
      ],
      "metadata": {
        "id": "A8bWJ0YmiutW",
        "outputId": "f09a94fb-3788-4df5-d102-1326324aba95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GL'...\n",
            "remote: Enumerating objects: 107, done.\u001b[K\n",
            "remote: Counting objects: 100% (107/107), done.\u001b[K\n",
            "remote: Compressing objects: 100% (76/76), done.\u001b[K\n",
            "remote: Total 107 (delta 33), reused 97 (delta 26), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (107/107), 646.55 KiB | 3.92 MiB/s, done.\n",
            "Resolving deltas: 100% (33/33), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PDFPlumberLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Make sure the PDF path is correct\n",
        "pdf_path = r\"/content/GL/LLM/RAG/Basic_Home_Remedies.pdf\"\n",
        "loader = PDFPlumberLoader(pdf_path)\n",
        "docs = loader.load()\n",
        "\n",
        "print(f\"Number of pages in the PDF: {len(docs)}\")\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
        ")\n",
        "documents = text_splitter.split_documents(docs)\n",
        "print(f\"Number of document chunks created: {len(documents)}\")"
      ],
      "metadata": {
        "id": "8I_aUnvdi0zb",
        "outputId": "bd636748-9b67-4379-b39f-0173b2adda9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain_community'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3425080482.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_loaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPDFPlumberLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_splitter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRecursiveCharacterTextSplitter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Make sure the PDF path is correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpdf_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"/content/GL/LLM/RAG/Basic_Home_Remedies.pdf\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_community'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from google.colab import userdata\n",
        "import time\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_pinecone import Pinecone as PineconeVectorStore\n",
        "\n",
        "# Securely get the API key from Colab secrets\n",
        "try:\n",
        "    PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\"ERROR: PINECONE_API_KEY not found in Colab secrets.\")\n",
        "    print(\"Please follow Step 1 in the instructions to add your key.\")\n",
        "    # Exit the script if the key is not found\n",
        "    exit()\n",
        "\n",
        "index_name = \"Medical-rag\"\n",
        "\n",
        "# Initialize the Pinecone client\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "print(f\"--- Pinecone initialized for index: '{index_name}' ---\")"
      ],
      "metadata": {
        "id": "jCVwJIEijDu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the embedding model (same as your code)\n",
        "embedder = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\n",
        "\n",
        "# Get the dimension of the embeddings\n",
        "embedding_dim = len(embedder.embed_query(\"test query\"))\n",
        "\n",
        "# Create the index if it doesn't already exist\n",
        "index_name = \"medical-rag\" # Changed index name to lowercase and hyphen\n",
        "\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    print(f\"Creating new Pinecone index: {index_name}\")\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=embedding_dim,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud='aws', region='us-east-1')\n",
        "    )\n",
        "    print(\"Index created successfully.\")\n",
        "else:\n",
        "    print(f\"Index '{index_name}' already exists.\")"
      ],
      "metadata": {
        "id": "BYM_VDrMjO-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d777abe2"
      },
      "source": [
        "import os\n",
        "\n",
        "print(\"\\nCreating Pinecone vector store and populating it with documents...\")\n",
        "\n",
        "# Set the Pinecone API key as an environment variable\n",
        "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
        "\n",
        "# The from_documents method will create embeddings and upsert them into the Pinecone index\n",
        "vector_store = PineconeVectorStore.from_documents(\n",
        "    documents,\n",
        "    embedder,\n",
        "    index_name=index_name\n",
        ")\n",
        "\n",
        "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})\n",
        "print(\"Vector store and retriever are ready.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ca47427"
      },
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "llm = ChatOpenAI(openai_api_base = \"https://openrouter.ai/api/v1\", openai_api_key = \"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\", model = \"deepseek/deepseek-chat-v3.1:free\",temperature=0.7) #max_tokens=100\n",
        "\n",
        "# Your custom prompt template\n",
        "prompt = \"\"\"1. You are a doctor.\n",
        "2. Use the following pieces of context to answer the question at the end.\n",
        "3. Answer only by using the context and articulate it better, use bullet points and emoji if required.\n",
        "4. Keep the answer crisp and limited to 3-4 sentences.\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "Answer to the question:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)\n",
        "\n",
        "# Your chain setup\n",
        "llm_chain = LLMChain(llm=llm, prompt=QA_CHAIN_PROMPT, verbose=False)\n",
        "document_prompt = PromptTemplate(\n",
        "    input_variables=[\"page_content\", \"source\"],\n",
        "    template=\"Context:\\ncontent:{page_content}\\nsource:{source}\",\n",
        ")\n",
        "combine_documents_chain = StuffDocumentsChain(\n",
        "    llm_chain=llm_chain,\n",
        "    document_variable_name=\"context\",\n",
        "    document_prompt=document_prompt,\n",
        ")\n",
        "qa = RetrievalQA(\n",
        "    combine_documents_chain=combine_documents_chain,\n",
        "    verbose=True,\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Sending query to the RAG chain ---\")\n",
        "result = qa(\"remedy for cough?\")\n",
        "print(\"\\n--- Final Answer ---\")\n",
        "print(result[\"result\"])\n",
        "print(\"\\n--- Retrieved Documents ---\")\n",
        "for doc in result[\"source_documents\"]:\n",
        "    print(f\"Content: {doc.page_content}\\nSource: {doc.metadata.get('source', 'N/A')}\\n---\")"
      ],
      "metadata": {
        "id": "95PcKlMUkHtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Cleaning up. Deleting the index... ---\")\n",
        "pc.delete_index(index_name)\n",
        "print(f\"Index '{index_name}' has been deleted.\")"
      ],
      "metadata": {
        "id": "SIs4QskOl8GE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}