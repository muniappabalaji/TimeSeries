{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muniappabalaji/TimeSeries/blob/main/LLM/Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ef6f457",
      "metadata": {
        "id": "2ef6f457"
      },
      "source": [
        "Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0cb00e5",
      "metadata": {
        "id": "d0cb00e5"
      },
      "outputs": [],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a47c30ea",
      "metadata": {
        "id": "a47c30ea"
      },
      "outputs": [],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ab4bf67",
      "metadata": {
        "id": "2ab4bf67"
      },
      "source": [
        "Connect to LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c892c928",
      "metadata": {
        "id": "c892c928"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(openai_api_base = \"http://localhost:1234/v1\", openai_api_key = \"lm_studio\", model = \"qwen3-0.6b\",temperature=0.9) #max_tokens=100"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35faadf7",
      "metadata": {
        "id": "35faadf7"
      },
      "source": [
        "Remove thinking Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b285f70",
      "metadata": {
        "id": "5b285f70"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def remove_think_sections(text):\n",
        "    return re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86516d9d",
      "metadata": {
        "id": "86516d9d"
      },
      "outputs": [],
      "source": [
        "text=\"What would be a good company name for a company that makes colorful socks?\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f0b4dee",
      "metadata": {
        "id": "1f0b4dee"
      },
      "source": [
        "Basic Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84e2a7a5",
      "metadata": {
        "id": "84e2a7a5"
      },
      "outputs": [],
      "source": [
        "output = llm.predict(text)\n",
        "output = remove_think_sections(output)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfb43815",
      "metadata": {
        "id": "dfb43815"
      },
      "source": [
        "Prompt Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "415b8528",
      "metadata": {
        "id": "415b8528"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=['cuisine'],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "formatted_prompt = prompt.format(cuisine=\"Italian\")\n",
        "print(formatted_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e17fe5d4",
      "metadata": {
        "id": "e17fe5d4"
      },
      "source": [
        "Few Shot Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc3cb52e",
      "metadata": {
        "id": "cc3cb52e"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import FewShotPromptTemplate\n",
        "\n",
        "examples = [\n",
        "    {\"word\": \"cat\", \"definition\": \"A small domesticated carnivorous mammal.\"},\n",
        "    {\"word\": \"dog\", \"definition\": \"A domesticated carnivorous mammal that typically has a long snout.\"},\n",
        "]\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"word\", \"definition\"],\n",
        "    template=\"Word: {word}\\nDefinition: {definition}\\n\"\n",
        ")\n",
        "\n",
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"Define the following words:\",\n",
        "    suffix=\"Word: {input}\\nDefinition:\",\n",
        "    input_variables=[\"input\"],\n",
        ")\n",
        "\n",
        "output = (few_shot_prompt.format(input=\"elephant\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0a42b66",
      "metadata": {
        "id": "e0a42b66"
      },
      "outputs": [],
      "source": [
        "output = llm.predict(output)\n",
        "output = remove_think_sections(output)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9875af3f",
      "metadata": {
        "id": "9875af3f"
      },
      "source": [
        "Simple Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6d87ccd",
      "metadata": {
        "id": "a6d87ccd",
        "outputId": "c70b5018-2622-4958-95d8-65ab44f23e55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "ðŸš€ AI in Education: Revolutionizing Learning ðŸ§ ðŸ’¡  \n",
            "Transform classrooms with personalized learning and smarter instructionâ€”AI is here to make education smarter, more accessible, and brighter for everyone!  \n",
            "\n",
            "Let me know if you'd like a variation or a call to action! ðŸ’¥ #EducationFuture #AIInSchool\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Give me a tweet idea about {topic}\"\n",
        ")\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "response = chain.run(\"AI in education\")\n",
        "response = remove_think_sections(response)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f54f0d45",
      "metadata": {
        "id": "f54f0d45"
      },
      "source": [
        "Sequential Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d55046d",
      "metadata": {
        "id": "2d55046d",
        "outputId": "be4ff563-6c4c-41c5-8685-04f3da496305"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'topic': 'LangChain for AI-powered apps', 'title': '<think>\\nOkay, the user wants me to write a compelling blog post title about LangChain for AI-powered apps. Let me start by understanding what LangChain is. It\\'s an open-source platform designed to help developers build applications that use language models effectively. Now, I need to make sure my title captures both its benefits and the target audience.\\n\\nFirst, \"LangChain\" is a key part of the title. Maybe highlight how it simplifies AI integration for developers? Then mention the focus on AI-powered apps. Words like \"revolutionize,\" \"transform,\" or \"drive\" could work here. Also, considering the audienceâ€”probably developers who want to create impactful applications using NLP tools.\\n\\nI should avoid being too technical but still convey excitement. Maybe something that combines innovation with practicality. Let me brainstorm a few options: \"Empowering Developers with LangChain for AI-Powered Applications.\" Or \"LangChain: How AI-Powered Apps Will Transform Your Workflow.\" Wait, the user might prefer something more catchy and engaging.\\n\\nI need to ensure it\\'s concise but still comprehensive. Including keywords like \"revolutionize\" or \"transform\" could make it stand out. Also, mentioning real-world benefits like improving efficiency or user experience would add value. Let me check for repetition and clarity. Maybe combine both the platform and its impact on developers.\\n</think>\\n\\n**\"LangChain: Revolutionize AI-Powered Applications with Simplified Integration That Empowers Developers.\"**  \\n\\nThis title blends innovation and practicality, emphasizing LangChainâ€™s role in simplifying AI integration and driving development excellence. It invites developers to explore how the platform transforms their approach to building AI-driven apps, while maintaining a focus on efficiency and impact.', 'intro': '<think>  \\nAlright, let me start by understanding the user\\'s query. They want a short blog introduction for a post titled \"LangChain.\" First, I need to make sure I grasp what LangChain is. It\\'s an open-source platform designed for building AI-powered applications with language models.\\n\\nNext, the target audience is likely developers who are looking to integrate advanced NLP tools into their existing projects without significant complexity. They might be aiming for efficiency, scalability, or a new level of functionality. \\n\\nI should focus on highlighting LangChainâ€™s strengths. The title mentions \"simplifies integration,\" which is crucial because many AI platforms require extensive setup and configuration. Emphasizing how it makes the process easier could resonate with developers looking to streamline their workflows. \\n\\nThe user also wants the introduction to be engaging and concise. Words like \"transform\" or \"revolutionize\" can create an exciting tone, making the post more appealing. Including keywords related to AI-driven applications will help the audience identify the postâ€™s relevance. \\n\\nI need to avoid jargon while still conveying technical value. Maybe mention benefits like improved efficiency or user experience without getting too deep into specifics. The title should capture both innovation and practicality, ensuring it appeals to a broad audience. Let me double-check for clarity and make sure it\\'s a strong opener for the blog post.'}\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import SequentialChain\n",
        "\n",
        "# Prompt 1: Generate title\n",
        "title_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Write a compelling blog post title about {topic}\"\n",
        ")\n",
        "title_chain = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")\n",
        "\n",
        "# Prompt 2: Use title to generate intro\n",
        "intro_prompt = PromptTemplate(\n",
        "    input_variables=[\"title\"],\n",
        "    template=\"Write a short blog introduction for the post titled: {title}\"\n",
        ")\n",
        "intro_chain = LLMChain(llm=llm, prompt=intro_prompt, output_key=\"intro\")\n",
        "\n",
        "# Chain them together\n",
        "overall_chain = SequentialChain(\n",
        "    chains=[title_chain, intro_chain],\n",
        "    input_variables=[\"topic\"],\n",
        "    output_variables=[\"title\", \"intro\"]\n",
        ")\n",
        "\n",
        "result = overall_chain.invoke(\"LangChain for AI-powered apps\")\n",
        "result = remove_think_sections(result)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe2ee2c7",
      "metadata": {
        "id": "fe2ee2c7"
      },
      "source": [
        "Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5db39874",
      "metadata": {
        "id": "5db39874"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(openai_api_base = \"http://localhost:1234/v1\", openai_api_key = \"lm_studio\", model = \"llama-3.2-1b-instruct\",temperature=0.9) #max_tokens=100\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True,  # shows the input/output/logs\n",
        ")\n",
        "\n",
        "print (conversation.predict(input=\"Hi, my name is Deepan.\"))\n",
        "\n",
        "\n",
        "print (conversation.predict(input=\"Whatâ€™s my name?\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e02088c1",
      "metadata": {
        "id": "e02088c1"
      },
      "source": [
        "Custom Parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04332596",
      "metadata": {
        "id": "04332596"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.schema import BaseOutputParser\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(openai_api_base = \"http://localhost:1234/v1\", openai_api_key = \"lm_studio\", model = \"qwen3-0.6b\",temperature=0.9) #max_tokens=100\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "class RemoveThinkParser(BaseOutputParser):\n",
        "    def parse(self, text: str) -> str:\n",
        "        # remove <think>...</think> blocks\n",
        "        return re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True,\n",
        "    output_parser=RemoveThinkParser()\n",
        ")\n",
        "\n",
        "print (conversation.predict(input=\"Hi, my name is Deepan.\"))\n",
        "\n",
        "\n",
        "print (conversation.predict(input=\"Whatâ€™s my name?\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea138374",
      "metadata": {
        "id": "ea138374"
      },
      "source": [
        "Custom Prompt template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bb86e93",
      "metadata": {
        "id": "6bb86e93"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.schema import BaseOutputParser\n",
        "\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "class RemoveThinkParser(BaseOutputParser):\n",
        "    def parse(self, text: str) -> str:\n",
        "        # remove <think>...</think> blocks\n",
        "        return re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
        "\n",
        "custom_prompt = PromptTemplate(\n",
        "    input_variables=[\"history\", \"input\"],\n",
        "    template=\"\"\"\n",
        "You are a helpful and concise assistant.\n",
        "\n",
        "{history}\n",
        "Human: {input}\n",
        "AI:\"\"\"\n",
        ")\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True,\n",
        "    prompt=custom_prompt,\n",
        "    output_parser=RemoveThinkParser()\n",
        ")\n",
        "\n",
        "\n",
        "print (conversation.predict(input=\"Hi, my name is Deepan.\"))\n",
        "\n",
        "\n",
        "print (conversation.predict(input=\"Whatâ€™s my name?\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bab91639",
      "metadata": {
        "id": "bab91639"
      },
      "source": [
        "Conversation Summary Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac278a2c",
      "metadata": {
        "id": "ac278a2c"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.schema import BaseOutputParser\n",
        "\n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "\n",
        "llm = ChatOpenAI(openai_api_base = \"http://localhost:1234/v1\", openai_api_key = \"lm_studio\", model = \"llama-3.2-1b-instruct\",temperature=0.9) #max_tokens=100\n",
        "\n",
        "\n",
        "summary_memory = ConversationSummaryMemory(llm=llm)\n",
        "\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "class RemoveThinkParser(BaseOutputParser):\n",
        "    def parse(self, text: str) -> str:\n",
        "        # remove <think>...</think> blocks\n",
        "        return re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
        "\n",
        "custom_prompt = PromptTemplate(\n",
        "    input_variables=[\"history\", \"input\"],\n",
        "    template=\"\"\"\n",
        "You are a helpful and concise assistant.\n",
        "\n",
        "{history}\n",
        "Human: {input}\n",
        "AI:\"\"\"\n",
        ")\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=summary_memory,\n",
        "    verbose=True,\n",
        "    prompt=custom_prompt,\n",
        "    output_parser=RemoveThinkParser()\n",
        ")\n",
        "\n",
        "\n",
        "print (conversation.predict(input=\"Hi, my name is Deepan.\"))\n",
        "\n",
        "\n",
        "print (conversation.predict(input=\"Whatâ€™s my name?\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3f25d8d",
      "metadata": {
        "id": "a3f25d8d"
      },
      "source": [
        "Memory with Chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6e4c580",
      "metadata": {
        "id": "c6e4c580"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"input\"],\n",
        "    template=\"The user said: {input}\"\n",
        ")\n",
        "\n",
        "chain_with_memory = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    memory=ConversationBufferMemory()\n",
        ")\n",
        "\n",
        "chain_with_memory.predict(input=\"Hello there!\")\n",
        "chain_with_memory.predict(input=\"What did I just say?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d42278a",
      "metadata": {
        "id": "0d42278a"
      },
      "outputs": [],
      "source": [
        "!pip install google-search-results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6512e61",
      "metadata": {
        "id": "f6512e61"
      },
      "outputs": [],
      "source": [
        "!pip install numexpr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dad9c45",
      "metadata": {
        "id": "2dad9c45"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.agents import AgentType, initialize_agent, load_tools\n",
        "\n",
        "# set serpapi key\n",
        "os.environ[\"SERPAPI_API_KEY\"] = \"42a63592c032fc7a97f972c068c18c2055f7409b1886d53270e4e77d089fd71f\"\n",
        "\n",
        "# local model\n",
        "llm = ChatOpenAI(\n",
        "    openai_api_base=\"http://localhost:1234/v1\",\n",
        "    openai_api_key=\"lm_studio\",\n",
        "    model=\"llama-3.2-1b-instruct\",\n",
        "    temperature=0.9\n",
        ")\n",
        "\n",
        "# tools\n",
        "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
        "\n",
        "# agent\n",
        "agent = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True\n",
        ")\n",
        "\n",
        "# test\n",
        "print(agent.run(\"What was the GDP of US in 2022 plus 5?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61cd7e7f",
      "metadata": {
        "id": "61cd7e7f"
      },
      "outputs": [],
      "source": [
        "!pip install wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e1d66a4",
      "metadata": {
        "id": "0e1d66a4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.agents import AgentType, initialize_agent, load_tools\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    openai_api_base=\"http://localhost:1234/v1\",\n",
        "    openai_api_key=\"lm_studio\",\n",
        "    model=\"llama-3.2-1b-instruct\",\n",
        "    temperature=0.9\n",
        ")\n",
        "\n",
        "# tools (wikipedia + math)\n",
        "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
        "\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True\n",
        ")\n",
        "\n",
        "\n",
        "print(agent.run(\"In what year was the film Departed with Leonardo DiCaprio released? \"\n",
        "                ))\n",
        "\n",
        "# \"What is this year raised to the 0.43 power?\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18d65c5c",
      "metadata": {
        "id": "18d65c5c"
      },
      "source": [
        "Callback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1014d3e7",
      "metadata": {
        "id": "1014d3e7"
      },
      "outputs": [],
      "source": [
        "from langchain.callbacks.base import BaseCallbackHandler\n",
        "\n",
        "class MyCustomHandler(BaseCallbackHandler):\n",
        "    def on_llm_start(self, serialized, prompts, **kwargs):\n",
        "        print(f\"LLM is starting with prompt: {prompts}\")\n",
        "\n",
        "    def on_llm_end(self, response, **kwargs):\n",
        "        print(f\"LLM finished. Response: {response}\")\n",
        "\n",
        "    def on_llm_error(self, error, **kwargs):\n",
        "        print(f\"LLM errored: {error}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d2bda28",
      "metadata": {
        "id": "8d2bda28"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    openai_api_base=\"http://localhost:1234/v1\",\n",
        "    openai_api_key=\"lm_studio\",\n",
        "    model=\"llama-3.2-1b-instruct\",\n",
        "    callbacks=[MyCustomHandler()]\n",
        ")\n",
        "\n",
        "print(llm.predict(\"Hello!\"))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}